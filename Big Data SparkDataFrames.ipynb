{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81d79747-3c55-4f63-9dd1-2b5ee7dcba7d",
   "metadata": {},
   "source": [
    "# Spark DataFrames\n",
    "\n",
    "**Objectives:** Load and manipulate data using Spark SQL DataFrames; Describe the similarities and differences between RDDs, Spark SQL DataFrames, and pandas DataFrames\n",
    "\n",
    "Spark SQL is a Spark module for structured data processing. Unlike the basic Spark RDD API, the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed.\n",
    "\n",
    "\r\n",
    "### Understanding SparkSession\r\n",
    "\r\n",
    "In the previous lessons, we were using the Unstructured API and therefore we connected to Spark using a SparkContext. Here, we will be using SparkSession instead ([documentation here](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html)), which is actually wrapped around a SparkContext under the hood. SparkSession is designed for interacting with high-level Spark SQL data structures (e.g. DataFrames) whereas SparkContext is desedito interacting with low-level Spark Core data structures (e.g. RDDs).\r\n",
    "\r\n",
    "A SparkSession is created using a *builder* pattern ([documentation here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.html)) and its conventional name is `spark`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "872fb680-0ffc-4515-9ea8-decba7b73f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89599217-eb63-4931-abf8-05db95c461b9",
   "metadata": {},
   "source": [
    "### Creating a Spark SQL DataFrame with PySpark\n",
    "\n",
    "Now that we have a SparkSession, we can create a DataFrame using the `createDataFrame` method ([documentation here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.createDataFrame.html))! One way to do this would just be to hard-code the data using built-in Python types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c975d6b-5b20-4da1-ab18-8533ae73a7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msagu\\anaconda3\\envs\\spark-env\\lib\\site-packages\\pyspark\\sql\\session.py:378: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[color: string, count: bigint, number: bigint, valid: boolean]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df = spark.createDataFrame([\n",
    "    {\"color\": \"red\", \"number\": 4, \"count\": 1, \"valid\": True},\n",
    "    {\"color\": \"blue\", \"number\": 7, \"count\": 2, \"valid\": False},\n",
    "    {\"color\": \"green\", \"number\": 1, \"count\": 3, \"valid\": True}\n",
    "])\n",
    "spark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2580205-99e8-4aae-883f-239c8c26c057",
   "metadata": {},
   "source": [
    "When the notebook displays `spark_df`, it is showing the schema between the square brackets -- in other words, it is displaying the column names and data types. We did not specify explicit data types for the columns, so Spark inferred them for us.\n",
    "\n",
    "You can view a nice print-out of the schema like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "847e72a5-b969-49a3-b517-b020d5f2a6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- color: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- number: long (nullable = true)\n",
      " |-- valid: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67739e24-88f9-491b-895e-8032e5ec44af",
   "metadata": {},
   "source": [
    "### Basic Features of Spark SQL DataFrames\r\n",
    "\r\n",
    "#### RDD Methods\r\n",
    "\r\n",
    "Many of the familiar RDD methods will work with DataFrames.\r\n",
    "\r\n",
    "For example, `.collect()` to load all of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ed8560d-734f-4e6a-a866-1707a11f63d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(color='red', count=1, number=4, valid=True),\n",
       " Row(color='blue', count=2, number=7, valid=False),\n",
       " Row(color='green', count=3, number=1, valid=True)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8aaef8-9481-4f31-a7d5-ad8321a3594e",
   "metadata": {},
   "source": [
    "And `.take(n)` to return `n` rows of data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6704aa26-58fe-4cd8-8f52-fce16ef8b155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(color='red', count=1, number=4, valid=True),\n",
       " Row(color='blue', count=2, number=7, valid=False)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f9c763-50a2-4068-97f3-89cb053be54c",
   "metadata": {},
   "source": [
    "#### DataFrame-Specific Methods\n",
    "\n",
    "In addition to the methods that work on RDDs, there are methods specific to DataFrames.\n",
    "\n",
    "For example, we can view all of the data in a tabular format using `.show()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a270791-4687-4920-84af-818ede648de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+-----+\n",
      "|color|count|number|valid|\n",
      "+-----+-----+------+-----+\n",
      "|  red|    1|     4| true|\n",
      "| blue|    2|     7|false|\n",
      "|green|    3|     1| true|\n",
      "+-----+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccf9b02-7135-4c4c-a7c4-f676a8e7ae6a",
   "metadata": {},
   "source": [
    "(We will add a `.show()` to the end of most of the following examples, since it's easier to read that way.)\n",
    "\n",
    "If we want to select the data from one or more specific columns, we can use `.select()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a951e8e1-4fa8-44df-8d61-165c4d5560d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     4|\n",
      "|     7|\n",
      "|     1|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.select(\"number\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "116be72a-8a0a-4d0a-bf58-f3750356b383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|number|valid|\n",
      "+------+-----+\n",
      "|     4| true|\n",
      "|     7|false|\n",
      "|     1| true|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.select([\"number\", \"valid\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79a9909-395f-42f3-a3ed-1aa893c431b9",
   "metadata": {},
   "source": [
    "### Familiar Techniques for Pandas Developers\r\n",
    "\r\n",
    "There are also several attributes and methods that work very similarly for Spark SQL DataFrames as they do with pandas DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91226d61-c613-47c4-8ef4-74182b284b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['color', 'count', 'number', 'valid']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d4af450-f490-4e23-842a-81a1553df96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+------+\n",
      "|summary|color|count|number|\n",
      "+-------+-----+-----+------+\n",
      "|  count|    3|    3|     3|\n",
      "|   mean| null|  2.0|   4.0|\n",
      "| stddev| null|  1.0|   3.0|\n",
      "|    min| blue|    1|     1|\n",
      "|    max|  red|    3|     7|\n",
      "+-------+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bda862-276b-4ff5-b2ee-751ce9bf2f00",
   "metadata": {},
   "source": [
    "## Loading the Forest Fire Dataset\n",
    "\n",
    "For this example, we're going to be using the [Forest Fire dataset](https://archive.ics.uci.edu/ml/datasets/Forest+Fires) from UCI, which contains data about the area burned by wildfires in the Northeast region of Portugal in relation to numerous other factors.\n",
    " \n",
    "We'll use `spark.read.csv` to load in the dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0393785-04b0-45c8-9879-47e7581e24d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+---+----+----+-----+---+----+---+----+----+----+\n",
      "|  X|  Y|month|day|FFMC| DMC|   DC|ISI|temp| RH|wind|rain|area|\n",
      "+---+---+-----+---+----+----+-----+---+----+---+----+----+----+\n",
      "|  7|  5|  mar|fri|86.2|26.2| 94.3|5.1| 8.2| 51| 6.7| 0.0| 0.0|\n",
      "|  7|  4|  oct|tue|90.6|35.4|669.1|6.7|18.0| 33| 0.9| 0.0| 0.0|\n",
      "|  7|  4|  oct|sat|90.6|43.7|686.9|6.7|14.6| 33| 1.3| 0.0| 0.0|\n",
      "|  8|  6|  mar|fri|91.7|33.3| 77.5|9.0| 8.3| 97| 4.0| 0.2| 0.0|\n",
      "|  8|  6|  mar|sun|89.3|51.3|102.2|9.6|11.4| 99| 1.8| 0.0| 0.0|\n",
      "+---+---+-----+---+----+----+-----+---+----+---+----+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_df = spark.read.csv(\"forestfires.csv\", header=\"true\", inferSchema=\"true\")\n",
    "fire_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cb5959-881e-4c46-8026-168b2a2966af",
   "metadata": {},
   "source": [
    "### Spark DataFrame Aggregations\n",
    "\n",
    "Let's investigate to see if there is any relationship between what month it is and the area of fire.\n",
    "\n",
    "First we'll group by the `month` column, then aggregate based on the mean of the `area` column for that group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "652def17-7e18-47b4-b807-9de036f69c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[month: string, avg(area): double]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fire_df_months = fire_df.groupBy('month').agg({'area': 'mean'})\n",
    "fire_df_months"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811db057-231a-4142-917d-8018da85ba9d",
   "metadata": {},
   "source": [
    "Notice how the grouped DataFrame is not returned when you call the aggregation method. Remember, this is still Spark! The transformations and actions are kept separate so that it is easier to manage large quantities of data. You can perform the transformation by calling `.collect()`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08744272-590a-4e2b-add5-0bfa8fab4be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(month='jun', avg(area)=5.841176470588234),\n",
       " Row(month='aug', avg(area)=12.489076086956521),\n",
       " Row(month='may', avg(area)=19.24),\n",
       " Row(month='feb', avg(area)=6.275),\n",
       " Row(month='sep', avg(area)=17.942616279069753),\n",
       " Row(month='mar', avg(area)=4.356666666666667),\n",
       " Row(month='oct', avg(area)=6.638),\n",
       " Row(month='jul', avg(area)=14.3696875),\n",
       " Row(month='nov', avg(area)=0.0),\n",
       " Row(month='apr', avg(area)=8.891111111111112),\n",
       " Row(month='dec', avg(area)=13.33),\n",
       " Row(month='jan', avg(area)=0.0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fire_df_months.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f072ddfb-fc5d-4653-9fa9-67af9bc17759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|month|         avg(area)|\n",
      "+-----+------------------+\n",
      "|  nov|               0.0|\n",
      "|  jan|               0.0|\n",
      "|  mar| 4.356666666666667|\n",
      "|  jun| 5.841176470588234|\n",
      "|  feb|             6.275|\n",
      "|  oct|             6.638|\n",
      "|  apr| 8.891111111111112|\n",
      "|  aug|12.489076086956521|\n",
      "|  dec|             13.33|\n",
      "|  jul|        14.3696875|\n",
      "|  sep|17.942616279069753|\n",
      "|  may|             19.24|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Let's show that as a table instead, and also order by the average area of fire:\n",
    "fire_df_months.orderBy(\"avg(area)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301dbfb2-45e6-471e-b012-b9cf18d86ddc",
   "metadata": {},
   "source": [
    "**As you can see, there seem to be larger area fires during what would be considered the summer months in Portugal. On your own, practice more aggregations and manipulations that you might be able to perform on this dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d0e48a-a601-4506-945a-c08ffdde59c9",
   "metadata": {},
   "source": [
    "## Boolean Masking \r\n",
    "\r\n",
    "Boolean masking also works with PySpark DataFrames just like Pandas DataFrames, the only difference being that the `.filter()` method is used in PySpark. To try this out, let's compare the amount of fire in those areas with absolutely no rain to those areas that had rain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f16187f-7f09-47b3-816e-96ca5af15627",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_rain = fire_df.filter(fire_df['rain'] == 0.0)\n",
    "some_rain = fire_df.filter(fire_df['rain'] > 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4709f010-88bd-451f-b2cc-8c77d1d0f6ff",
   "metadata": {},
   "source": [
    "Now, to perform calculations to find the mean of a column (without aggregating first), we'll have to import functions from `pyspark.sql`. As always, to read more about them, check out the [documentation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ecf7713-12e7-4397-b06d-d037cebad2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no rain fire area:\n",
      "+------------------+\n",
      "|         avg(area)|\n",
      "+------------------+\n",
      "|13.023693516699408|\n",
      "+------------------+\n",
      "\n",
      "some rain fire area:\n",
      "+---------+\n",
      "|avg(area)|\n",
      "+---------+\n",
      "|  1.62375|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "print('no rain fire area:')\n",
    "no_rain.select(mean('area')).show()\n",
    "\n",
    "print('some rain fire area:')\n",
    "some_rain.select(mean('area')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201cdb0b-2853-4775-bac9-a514c8e26ed0",
   "metadata": {},
   "source": [
    "Yes there's definitely something there! Unsurprisingly, rain plays in a big factor in the spread of wildfire.\n",
    "\n",
    "Let's obtain data from only the summer months in Portugal (June, July, and August). We can also do the same for the winter months in Portugal (December, January, February)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d5602eb-2a5b-4b6a-a2be-3253aa814abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summer months fire area:\n",
      "+------------------+\n",
      "|         avg(area)|\n",
      "+------------------+\n",
      "|12.262317596566525|\n",
      "+------------------+\n",
      "\n",
      "winter months fire area\n",
      "+-----------------+\n",
      "|        avg(area)|\n",
      "+-----------------+\n",
      "|7.918387096774193|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summer_months = fire_df.filter(fire_df['month'].isin(['jun','jul','aug']))\n",
    "winter_months = fire_df.filter(fire_df['month'].isin(['dec','jan','feb']))\n",
    "\n",
    "print('summer months fire area:')\n",
    "summer_months.select(mean('area')).show()\n",
    "\n",
    "print('winter months fire area')\n",
    "winter_months.select(mean('area')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfaf9a4-8128-4c3e-8040-28e9240060f8",
   "metadata": {},
   "source": [
    "## Comparison between DataFrames\r\n",
    "\r\n",
    "Although Spark SQL DataFrames and pandas DataFrames have some features in common, they are not the same. We'll demonstrate some similarities and differences below.\r\n",
    "\r\n",
    "First, we'll create a `pandas_df` with the same data as `spark_df` and compare the two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa9a4db5-98e7-4c52-bc35-5ef72516f520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.0.3-cp38-cp38-win_amd64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\msagu\\anaconda3\\envs\\spark-env\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\msagu\\anaconda3\\envs\\spark-env\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "     ---------------------------------------- 0.0/341.8 kB ? eta -:--:--\n",
      "     ------------- ------------------------ 122.9/341.8 kB 7.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- 341.8/341.8 kB 7.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\msagu\\anaconda3\\envs\\spark-env\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\msagu\\anaconda3\\envs\\spark-env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.0.3-cp38-cp38-win_amd64.whl (10.8 MB)\n",
      "   ---------------------------------------- 0.0/10.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.7/10.8 MB 21.5 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 1.5/10.8 MB 18.9 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.5/10.8 MB 17.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.5/10.8 MB 20.5 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.6/10.8 MB 19.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.7/10.8 MB 21.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.9/10.8 MB 20.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.0/10.8 MB 21.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.3/10.8 MB 22.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.6/10.8 MB 23.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.8/10.8 MB 21.8 MB/s eta 0:00:00\n",
      "Installing collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.0.3 tzdata-2023.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b30f9da3-c223-419d-8ae4-49df74df7329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pandas_df = pd.DataFrame([\n",
    "    {\"color\": \"red\", \"number\": 4, \"count\": 1, \"valid\": True},\n",
    "    {\"color\": \"blue\", \"number\": 7, \"count\": 2, \"valid\": False},\n",
    "    {\"color\": \"green\", \"number\": 1, \"count\": 3, \"valid\": True}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "038d7975-7f0d-4023-9a2c-169860bd6efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>number</th>\n",
       "      <th>count</th>\n",
       "      <th>valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>red</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blue</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>green</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   color  number  count  valid\n",
       "0    red       4      1   True\n",
       "1   blue       7      2  False\n",
       "2  green       1      3   True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display data\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "898715f1-7430-4e7c-9344-e1ff73b9a30f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[color: string, count: bigint, number: bigint, valid: boolean]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596d66ef-626c-492f-a5f7-3f0d091dadc0",
   "metadata": {},
   "source": [
    "One difference you'll notice immediately, which has been pointed out a couple times in this lesson already, is that a Spark DataFrame loads lazily but a pandas DataFrame does not. Therefore if we just type the name of the variable we see at least a preview of the pandas data, but no preview of the Spark data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2a1f5c3-2b16-4659-9988-955b72320524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "color     object\n",
       "number     int64\n",
       "count      int64\n",
       "valid       bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Attributes\n",
    "pandas_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c1cba6c-951b-4c79-b048-6485f03ee8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('color', 'string'),\n",
       " ('count', 'bigint'),\n",
       " ('number', 'bigint'),\n",
       " ('valid', 'boolean')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f673c9d-720e-4cf4-a43e-f6a665205202",
   "metadata": {},
   "source": [
    "Sometimes a Spark DataFrame will have an attribute with a familiar name, but it won't work quite the same way. For example, the `dtypes` attribute is present for both, but in pandas it returns a Series and in Spark SQL it returns a list of tuples.\r\n",
    "\r\n",
    "The actual data types listed are also different, although they correspond to each other. For example, the `object` data type in pandas corresponds to the `string` data type in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f7388c8-8cc6-4174-8e12-baa622ee5176",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'red'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpandas_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\spark-env\\lib\\site-packages\\pandas\\core\\frame.py:10054\u001b[0m, in \u001b[0;36mDataFrame.corr\u001b[1;34m(self, method, min_periods, numeric_only)\u001b[0m\n\u001b[0;32m  10052\u001b[0m cols \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m  10053\u001b[0m idx \u001b[38;5;241m=\u001b[39m cols\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m> 10054\u001b[0m mat \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m  10056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpearson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m  10057\u001b[0m     correl \u001b[38;5;241m=\u001b[39m libalgos\u001b[38;5;241m.\u001b[39mnancorr(mat, minp\u001b[38;5;241m=\u001b[39mmin_periods)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\spark-env\\lib\\site-packages\\pandas\\core\\frame.py:1838\u001b[0m, in \u001b[0;36mDataFrame.to_numpy\u001b[1;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m   1836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1837\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(dtype)\n\u001b[1;32m-> 1838\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dtype:\n\u001b[0;32m   1840\u001b[0m     result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(result, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\spark-env\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1732\u001b[0m, in \u001b[0;36mBlockManager.as_array\u001b[1;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m   1730\u001b[0m         arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1731\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1732\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;66;03m# The underlying data was copied within _interleave, so no need\u001b[39;00m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;66;03m# to further copy if copy=True or setting na_value\u001b[39;00m\n\u001b[0;32m   1736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\spark-env\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1794\u001b[0m, in \u001b[0;36mBlockManager._interleave\u001b[1;34m(self, dtype, na_value)\u001b[0m\n\u001b[0;32m   1792\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1793\u001b[0m         arr \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mget_values(dtype)\n\u001b[1;32m-> 1794\u001b[0m     \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m arr\n\u001b[0;32m   1795\u001b[0m     itemmask[rl\u001b[38;5;241m.\u001b[39mindexer] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1797\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m itemmask\u001b[38;5;241m.\u001b[39mall():\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'red'"
     ]
    }
   ],
   "source": [
    "pandas_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e8b34ae-289b-4457-86ea-531d2e7c6d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.corr(\"number\", \"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2439b283-78da-406d-821d-d14cf1cbd8d5",
   "metadata": {},
   "source": [
    "In the example above, both have a `corr` method that is used for computing correlations between columns, but they work fairly differently.\n",
    "\n",
    "* The pandas method does not require any arguments and returns an entire DataFrame showing the correlations between all numeric variables (including `valid`, which contains booleans). \n",
    "* The Spark SQL method requires that you specify two column names and returns a single floating point number indicating the correlation between those two columns.\n",
    "\n",
    "Watch out for distinctions like this! And don't hesitate to read through the [Spark SQL documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#spark-sql) when you're trying out a new method.\n",
    "\n",
    "### Selecting Columns and Boolean Masking\n",
    "\n",
    "In pandas, you can select the data in a single column and it will be viewable as a Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f64dd057-3c8c-493b-80b2-28d1c08d0f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      red\n",
       "1     blue\n",
       "2    green\n",
       "Name: color, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df[\"color\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c11407-7b99-48f3-94ce-93b8a3d4b6a1",
   "metadata": {},
   "source": [
    "We can apply a method to that Series to get back a Series of booleans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8cac491-a1e3-427f-811d-0ad18638421e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     True\n",
       "1    False\n",
       "2     True\n",
       "Name: color, dtype: bool"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df[\"color\"].str.contains(\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef9e602-cd5c-41aa-8a7f-401de5a036a8",
   "metadata": {},
   "source": [
    "And then we can also use that Series of booleans to filter the DataFrame using boolean masking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83d3a1f5-1b0a-40c6-a1f4-91c40a4eeeba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>number</th>\n",
       "      <th>count</th>\n",
       "      <th>valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>red</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>green</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   color  number  count  valid\n",
       "0    red       4      1   True\n",
       "2  green       1      3   True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df[pandas_df[\"color\"].str.contains(\"r\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed058124-07c7-40f4-ac5c-a7d1bab616e8",
   "metadata": {},
   "source": [
    "In Spark, the intermediate Column data is not viewable and is only useful for applying the boolean mask.\n",
    "\n",
    "First we select the data in the `color` column, but if we try to `show()` the result, we get an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0da062e0-ec79-4ce0-827f-66fba636ad65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'color'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df[\"color\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d188a68-1888-4088-9019-68473c7436ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'TypeError'>\n",
      "'Column' object is not callable\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark_df[\"color\"].show()\n",
    "except Exception as e:\n",
    "    print(type(e))\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06052a9-bdf4-43ea-a44e-a53d4f6d725b",
   "metadata": {},
   "source": [
    "We can chain a `contains` method call onto the Column, but again, if we try to `show()` it, we get an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9e96fed-52fa-40ce-96e3-e034b4f1727c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'contains(color, r)'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df[\"color\"].contains(\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7c18f07-b67a-4906-8fca-440f3fd52f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'TypeError'>\n",
      "'Column' object is not callable\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark_df[\"color\"].contains(\"r\").show()\n",
    "except Exception as e:\n",
    "    print(type(e))\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3ff2cf-b60b-45df-9957-936fd018c1a6",
   "metadata": {},
   "source": [
    "If we want data we can show using this boolean mask, we have to apply the `filter` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5701242-0a08-445c-9a53-41fd20a7d570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[color: string, count: bigint, number: bigint, valid: boolean]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.filter(spark_df[\"color\"].contains(\"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0665ccf-c0e2-4856-84bb-37ef8c15d949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+-----+\n",
      "|color|count|number|valid|\n",
      "+-----+-----+------+-----+\n",
      "|  red|    1|     4| true|\n",
      "|green|    3|     1| true|\n",
      "+-----+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.filter(spark_df[\"color\"].contains(\"r\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9082d0-1149-498a-bde9-7e8e9c4ef8cc",
   "metadata": {},
   "source": [
    "### Converting Between DataFrame Types\n",
    "\n",
    "In general, it is not particularly efficient to convert from one DataFrame type to another. Usually the reason that you are using a Spark SQL DataFrame is that you are working with Big Data and require computational optimization, so it wouldn't make much sense to convert it to a pandas DataFrame.\n",
    "\n",
    "However you can imagine some specific circumstances where you might want to use pandas for debugging, visualization, or some other task that just isn't working in Spark SQL. If you need to do that, check out [this documentation](https://spark.apache.org/docs/latest/api/python/user_guide/sql/arrow_pandas.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7241dd7b-d023-424f-b98f-63599e851bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop the SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark-env)",
   "language": "python",
   "name": "spark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
